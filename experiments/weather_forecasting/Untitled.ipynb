{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21842ab6-615d-4614-8bca-edb277e03d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/torch/lib/libtorch_global_deps.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/torch/__init__.py:415\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Easy way.  You want this most of the time, because it will prevent\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# C++ symbols from libtorch clobbering C++ symbols from other\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# See Note [Global dependencies]\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m--> 415\u001b[0m         \u001b[43m_load_global_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/torch/__init__.py:371\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    367\u001b[0m is_cuda_lib_err \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    368\u001b[0m     lib \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m cuda_libs\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m err\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    369\u001b[0m ]\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cuda_lib_err:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lib_folder, lib_name \u001b[38;5;129;01min\u001b[39;00m cuda_libs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    373\u001b[0m     _preload_cuda_deps(lib_folder, lib_name)\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/torch/__init__.py:320\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m global_deps_lib_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(here), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, lib_name)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_deps_lib_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_GLOBAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# Workaround slim-wheel CUDA dependency bugs in cusparse and cudnn by preloading nvjitlink\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# and nvrtc. In CUDA-12.4+ cusparse depends on nvjitlink, but does not have rpath when\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# shipped as wheel, which results in OS picking wrong/older version of nvjitlink library\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;66;03m# if `LD_LIBRARY_PATH` is defined, see https://github.com/pytorch/pytorch/issues/138460\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# Similar issue exist in cudnn that dynamically loads nvrtc, unaware of its relative path.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/145580\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/torch/lib/libtorch_global_deps.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader, random_split\n",
    "from lightning import LightningDataModule, seed_everything\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea4eb0c-d92c-49be-a145-f0154ac34f86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m default_exps_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiments\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m default_dataset_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\"))\n",
    "default_exps_dir = os.path.abspath(os.path.join(root_dir, \"experiments\"))\n",
    "default_dataset_dir = os.path.abspath(os.path.join(root_dir, \"datasets\"))\n",
    "default_dataset_sevir_dir = os.path.abspath(os.path.join(default_dataset_dir, \"sevir\"))\n",
    "default_dataset_sevirlr_dir = os.path.abspath(os.path.join(default_dataset_dir, \"sevirlr\"))\n",
    "\n",
    "\n",
    "class SEVIRTorchDataset(TorchDataset):\n",
    "\n",
    "    orig_dataloader_layout = \"NHWT\"\n",
    "    orig_dataloader_squeeze_layout = orig_dataloader_layout.replace(\"N\", \"\")\n",
    "    aug_layout = \"THW\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 seq_len: int = 25,\n",
    "                 raw_seq_len: int = 49,\n",
    "                 sample_mode: str = \"sequent\",\n",
    "                 stride: int = 12,\n",
    "                 layout: str = \"THWC\",\n",
    "                 split_mode: str = \"uneven\",\n",
    "                 sevir_catalog: Union[str, pd.DataFrame] = None,\n",
    "                 sevir_data_dir: str = None,\n",
    "                 start_date: datetime.datetime = None,\n",
    "                 end_date: datetime.datetime = None,\n",
    "                 datetime_filter = None,\n",
    "                 catalog_filter = \"default\",\n",
    "                 shuffle: bool = False,\n",
    "                 shuffle_seed: int = 1,\n",
    "                 output_type = np.float32,\n",
    "                 preprocess: bool = True,\n",
    "                 rescale_method: str = \"01\",\n",
    "                 verbose: bool = False,\n",
    "                 aug_mode: str = \"0\",\n",
    "                 ret_contiguous: bool = True):\n",
    "        super(SEVIRTorchDataset, self).__init__()\n",
    "        self.layout = layout.replace(\"C\", \"1\")\n",
    "        self.ret_contiguous = ret_contiguous\n",
    "        self.sevir_dataloader = SEVIRDataLoader(\n",
    "            data_types=[\"vil\", ],\n",
    "            seq_len=seq_len,\n",
    "            raw_seq_len=raw_seq_len,\n",
    "            sample_mode=sample_mode,\n",
    "            stride=stride,\n",
    "            batch_size=1,\n",
    "            layout=self.orig_dataloader_layout,\n",
    "            num_shard=1,\n",
    "            rank=0,\n",
    "            split_mode=split_mode,\n",
    "            sevir_catalog=sevir_catalog,\n",
    "            sevir_data_dir=sevir_data_dir,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            datetime_filter=datetime_filter,\n",
    "            catalog_filter=catalog_filter,\n",
    "            shuffle=shuffle,\n",
    "            shuffle_seed=shuffle_seed,\n",
    "            output_type=output_type,\n",
    "            preprocess=preprocess,\n",
    "            rescale_method=rescale_method,\n",
    "            downsample_dict=None,\n",
    "            verbose=verbose)\n",
    "        self.aug_mode = aug_mode\n",
    "        if aug_mode == \"0\":\n",
    "            self.aug = lambda x:x\n",
    "        elif aug_mode == \"1\":\n",
    "            self.aug = nn.Sequential(\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(degrees=180),\n",
    "            )\n",
    "        elif aug_mode == \"2\":\n",
    "            self.aug = nn.Sequential(\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                TransformsFixRotation(angles=[0, 90, 180, 270]),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_dict = self.sevir_dataloader._idx_sample(index=index)\n",
    "        data = data_dict[\"vil\"].squeeze(0)\n",
    "        if self.aug_mode != \"0\":\n",
    "            data = rearrange(data, f\"{' '.join(self.orig_dataloader_squeeze_layout)} -> {' '.join(self.aug_layout)}\")\n",
    "            data = self.aug(data)\n",
    "            data = rearrange(data, f\"{' '.join(self.aug_layout)} -> {' '.join(self.layout)}\")\n",
    "        else:\n",
    "            data = rearrange(data, f\"{' '.join(self.orig_dataloader_squeeze_layout)} -> {' '.join(self.layout)}\")\n",
    "        # print('data', data.shape)\n",
    "        \n",
    "        # print('self.ret_contigous', self.ret_contiguous)\n",
    "        # assert 1==0\n",
    "        if self.ret_contiguous:\n",
    "            return data.contiguous()\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sevir_dataloader.__len__()\n",
    "\n",
    "\n",
    "class SEVIRLightningDataModule(LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 seq_len: int = 25,\n",
    "                 sample_mode: str = \"sequent\",\n",
    "                 stride: int = 12,\n",
    "                 layout: str = \"NTHWC\",\n",
    "                 output_type = np.float32,\n",
    "                 preprocess: bool = True,\n",
    "                 rescale_method: str = \"01\",\n",
    "                 verbose: bool = False,\n",
    "                 aug_mode: str = \"0\",\n",
    "                 ret_contiguous: bool = True,\n",
    "                 # datamodule_only\n",
    "                 dataset_name: str = \"sevir\",\n",
    "                 sevir_dir: str = None,\n",
    "                 start_date: Tuple[int] = None,\n",
    "                 train_test_split_date: Tuple[int] = (2019, 6, 1),\n",
    "                 end_date: Tuple[int] = None,\n",
    "                 val_ratio: float = 0.1,\n",
    "                 batch_size: int = 1,\n",
    "                 num_workers: int = 1,\n",
    "                 seed: int = 0,\n",
    "                 ):\n",
    "        super(SEVIRLightningDataModule, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.sample_mode = sample_mode\n",
    "        self.stride = stride\n",
    "        assert layout[0] == \"N\"\n",
    "        self.layout = layout.replace(\"N\", \"\")\n",
    "        self.output_type = output_type\n",
    "        self.preprocess = preprocess\n",
    "        self.rescale_method = rescale_method\n",
    "        self.verbose = verbose\n",
    "        self.aug_mode = aug_mode\n",
    "        self.ret_contiguous = ret_contiguous\n",
    "        self.batch_size = batch_size\n",
    "        print('batch_size', batch_size)\n",
    "        # assert 1==0\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        if sevir_dir is not None:\n",
    "            sevir_dir = os.path.abspath(sevir_dir)\n",
    "        if dataset_name == \"sevir\":\n",
    "            if sevir_dir is None:\n",
    "                sevir_dir = default_dataset_sevir_dir\n",
    "            catalog_path = os.path.join(sevir_dir, \"CATALOG.csv\")\n",
    "            raw_data_dir = os.path.join(sevir_dir, \"data\")\n",
    "            raw_seq_len = 49\n",
    "            interval_real_time = 5\n",
    "            img_height = 384\n",
    "            img_width = 384\n",
    "        elif dataset_name == \"sevirlr\":\n",
    "            if sevir_dir is None:\n",
    "                sevir_dir = default_dataset_sevirlr_dir\n",
    "            print('sevir_dir', sevir_dir)\n",
    "            catalog_path = os.path.join(sevir_dir, \"CATALOG.csv\")\n",
    "            raw_data_dir = os.path.join(sevir_dir, \"data\")\n",
    "            raw_seq_len = 25\n",
    "            interval_real_time = 10\n",
    "            img_height = 128\n",
    "            img_width = 128\n",
    "        else:\n",
    "            raise ValueError(f\"Wrong dataset name {dataset_name}. Must be 'sevir' or 'sevirlr'.\")\n",
    "        self.dataset_name = dataset_name\n",
    "        self.sevir_dir = sevir_dir\n",
    "        print(' self.sevir_dir',  self.sevir_dir)\n",
    "        self.catalog_path = catalog_path\n",
    "        self.raw_data_dir = raw_data_dir\n",
    "        self.raw_seq_len = raw_seq_len\n",
    "        self.interval_real_time = interval_real_time\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        # train val test split\n",
    "        self.start_date = datetime.datetime(*start_date) \\\n",
    "            if start_date is not None else None\n",
    "        self.train_test_split_date = datetime.datetime(*train_test_split_date) \\\n",
    "            if train_test_split_date is not None else None\n",
    "        self.end_date = datetime.datetime(*end_date) \\\n",
    "            if end_date is not None else None\n",
    "        self.val_ratio = val_ratio\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        if os.path.exists(self.sevir_dir):\n",
    "            # Further check\n",
    "            assert os.path.exists(self.catalog_path), f\"CATALOG.csv not found! Should be located at {self.catalog_path}\"\n",
    "            assert os.path.exists(self.raw_data_dir), f\"SEVIR data not found! Should be located at {self.raw_data_dir}\"\n",
    "        else:\n",
    "            if self.dataset_name == \"sevir\":\n",
    "                download_SEVIR(save_dir=os.path.dirname(self.sevir_dir))\n",
    "            elif self.dataset_name == \"sevirlr\":\n",
    "                download_SEVIRLR(save_dir=os.path.dirname(self.sevir_dir))\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    def setup(self, stage = None) -> None:\n",
    "        seed_everything(seed=self.seed)\n",
    "        if stage in (None, \"fit\"):\n",
    "            sevir_train_val = SEVIRTorchDataset(\n",
    "                sevir_catalog=self.catalog_path,\n",
    "                sevir_data_dir=self.raw_data_dir,\n",
    "                raw_seq_len=self.raw_seq_len,\n",
    "                split_mode=\"uneven\",\n",
    "                shuffle=True,\n",
    "                seq_len=self.seq_len,\n",
    "                stride=self.stride,\n",
    "                sample_mode=self.sample_mode,\n",
    "                layout=self.layout,\n",
    "                start_date=self.start_date,\n",
    "                end_date=self.train_test_split_date,\n",
    "                output_type=self.output_type,\n",
    "                preprocess=self.preprocess,\n",
    "                rescale_method=self.rescale_method,\n",
    "                verbose=self.verbose,\n",
    "                aug_mode=self.aug_mode,\n",
    "                ret_contiguous=self.ret_contiguous,)\n",
    "            \n",
    "\n",
    "            # Iterate through the dataset\n",
    "            \n",
    "\n",
    "            self.sevir_train, self.sevir_val = random_split(\n",
    "                dataset=sevir_train_val,\n",
    "                lengths=[1 - self.val_ratio, self.val_ratio],\n",
    "                generator=torch.Generator().manual_seed(self.seed))\n",
    "            \n",
    "        if stage in (None, \"test\"):\n",
    "            self.sevir_test = SEVIRTorchDataset(\n",
    "                sevir_catalog=self.catalog_path,\n",
    "                sevir_data_dir=self.raw_data_dir,\n",
    "                raw_seq_len=self.raw_seq_len,\n",
    "                split_mode=\"uneven\",\n",
    "                shuffle=False,\n",
    "                seq_len=self.seq_len,\n",
    "                stride=self.stride,\n",
    "                sample_mode=self.sample_mode,\n",
    "                layout=self.layout,\n",
    "                start_date=self.train_test_split_date,\n",
    "                end_date=self.end_date,\n",
    "                output_type=self.output_type,\n",
    "                preprocess=self.preprocess,\n",
    "                rescale_method=self.rescale_method,\n",
    "                verbose=self.verbose,\n",
    "                aug_mode=\"0\",\n",
    "                ret_contiguous=self.ret_contiguous,)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.sevir_train,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.sevir_val,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.sevir_test,\n",
    "                          batch_size=1,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    @property\n",
    "    def num_train_samples(self):\n",
    "        return len(self.sevir_train)\n",
    "\n",
    "    @property\n",
    "    def num_val_samples(self):\n",
    "        return len(self.sevir_val)\n",
    "\n",
    "    @property\n",
    "    def num_test_samples(self):\n",
    "        return len(self.sevir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7b37e-e361-4030-897e-a9bc191d8fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowdas",
   "language": "python",
   "name": "flowdas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
