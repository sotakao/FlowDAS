{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f31d0ec2-cc4f-4696-b6bf-53916666465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import functools\n",
    "import h5py\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import shutil\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from flowdas import ScoreNet, marginal_prob_std, Euler_Maruyama_sampler\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def prepare():\n",
    "    set_seed(427)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='FlowDAS Evaluation')\n",
    "    parser.add_argument('--config', type=str, default='eval_win1_G', \n",
    "                        help='Path to evaluation config')\n",
    "    parser.add_argument('--N_trajectory', type=int, default=64, \n",
    "                        help='Number of trajectories to evaluate')\n",
    "    parser.add_argument('--LT', type=int, default=15, \n",
    "                        help='Number of testing states of each trajectory')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    config_path = PATH / 'config' / f'{args.config}.yml'\n",
    "    config = get_config(config_path)\n",
    "\n",
    "    if args.N_trajectory is not None:\n",
    "        config['N_trajectory'] = args.N_trajectory\n",
    "    if args.LT is not None:\n",
    "        config['LT'] = args.LT\n",
    "\n",
    "    # Create a path that includes both timestamp and config name (without extension)\n",
    "    timestamp = datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "    runpath = PATH / 'runs_eval' / f'run_{timestamp}_{args.config}'\n",
    "    runpath.mkdir(parents=True, exist_ok=True)\n",
    "    setup_evaluation_logging(runpath)\n",
    "\n",
    "\n",
    "    # Log the configuration\n",
    "    logging.info(\"Evaluation configuration:\")\n",
    "    for key, value in config.items():\n",
    "        logging.info(f\"  {key}: {value}\")\n",
    "    \n",
    "    logging.info(\"\\n\\n\")    \n",
    "\n",
    "\n",
    "    # Update config with runpath and num_workers\n",
    "    # assert config['LT'] >= config['window'], \"LT must be greater than or equal to window\"\n",
    "    config['runpath'] = runpath\n",
    "    config['num_workers'] = int(subprocess.check_output(['nproc']).strip())\n",
    "    config['marginal_prob_std_fn'] = functools.partial(marginal_prob_std, sigma=config['sigma'])\n",
    "\n",
    "\n",
    "def get_flow_prior(config):\n",
    "    flow_prior = ScoreNet(\n",
    "        marginal_prob_std=config['marginal_prob_std_fn'],\n",
    "        x_dim=config['x_dim'],\n",
    "        extra_dim=config['extra_dim']*config['window'],\n",
    "        hidden_depth=config['hidden_depth'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        use_bn=config['use_bn']\n",
    "    ).to(config['device'])\n",
    "\n",
    "    try:\n",
    "        ckp_path = config[f'checkpoint_path']\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Window size {config['window']} not supported\")\n",
    "\n",
    "    flow_prior = load_checkpoint(flow_prior, ckp_path)\n",
    "    return flow_prior\n",
    "\n",
    "\n",
    "def _to_torch(arr_like, dtype=np.float32):\n",
    "    # 1) materialize as a real ndarray with desired dtype\n",
    "    arr = np.asarray(arr_like, dtype=dtype)\n",
    "\n",
    "    # 2) ensure writable + C-contiguous (HDF5 slices are often read-only)\n",
    "    if (not arr.flags['C_CONTIGUOUS']) or (not arr.flags['WRITEABLE']):\n",
    "        arr = arr.copy(order='C')  # makes it contiguous & writeable\n",
    "\n",
    "    # 3) try zero-copy; if PyTorch still complains, fall back to copy\n",
    "    try:\n",
    "        return torch.from_numpy(arr)\n",
    "    except TypeError:\n",
    "        # last resortâ€”copy into a fresh tensor (handles weird subclasses)\n",
    "        return torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def create_observations(config):\n",
    "    \"\"\"\n",
    "    Create observation files for the combined-para dataset.\n",
    "    \n",
    "    The obs.h5 file contains the first (L+1) steps of the testing trajectory.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary with parameters\n",
    "        \n",
    "    Input data shape: \n",
    "        x: (N, L+1, 3) - N trajectories with L+1 timesteps of 3D coordinates\n",
    "        \n",
    "    Output data shape:\n",
    "        obs: (N, L+1, 1) - Observations for each trajectory and timestep\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating observations...\")\n",
    "    path_dataset = config[f'path_dataset']\n",
    "    \n",
    "    # Define the observation file path\n",
    "    obs_file_path = f'{path_dataset}/obs_L{config[\"LT\"]}_win{config[\"window\"]}.h5'\n",
    "    \n",
    "    # Read input data\n",
    "    with h5py.File(f'{path_dataset}/test.h5', mode='r') as f:\n",
    "        x = f['data'][:, :config['LT']+config['window']]  # shape: (N, L+w, 3)\n",
    "    \n",
    "    # Check if the observation file already exists\n",
    "    if os.path.exists(obs_file_path):\n",
    "        # Generate what the new observations would be\n",
    "        # x_tensor = torch.from_numpy(x)\n",
    "        arr = x[...]            # shape e.g. (L, 3)\n",
    "        x_tensor = _to_torch(arr, dtype=np.float32)\n",
    "        new_obs = observation_generator(x_tensor, config['sigma_obs_hi'])\n",
    "        \n",
    "        # Load existing observations\n",
    "        with h5py.File(obs_file_path, mode='r') as f:\n",
    "            existing_obs = f['obs'][:]\n",
    "        \n",
    "        # Compare existing and new observations\n",
    "        # if np.array_equal(existing_obs, new_obs.numpy()):\n",
    "        if np.allclose(existing_obs, new_obs.numpy(), rtol=1e-3, atol=1e-3):\n",
    "            logging.info(f\"Existing observations file is identical. Skipping creation.\")\n",
    "        else:\n",
    "            logging.error(f\"Existing observations file contains different data!\")\n",
    "            logging.error(f\"This requires attention. Exiting evaluation.\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        # Create new observations file\n",
    "        with h5py.File(obs_file_path, mode='w') as f:\n",
    "            # x_tensor = torch.from_numpy(x)\n",
    "            arr = x[...]            # shape e.g. (L, 3)\n",
    "            x_tensor = _to_torch(arr, dtype=np.float32)\n",
    "            obs = observation_generator(x_tensor, config['sigma_obs_hi'])\n",
    "            f.create_dataset('obs', data=obs)\n",
    "            logging.info(f\"Created new observations file at {obs_file_path}\")\n",
    "\n",
    "    logging.info(\"Observations created successfully.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab55a411-528d-45d8-96d7-7088ff098f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SCRATCH' in os.environ:\n",
    "    SCRATCH = os.environ['SCRATCH']\n",
    "    PATH = Path(SCRATCH) / 'sda/lorenz'\n",
    "else:\n",
    "    PATH = Path('.')\n",
    "\n",
    "config_path = PATH / 'config' / f'eval_win1.yml'\n",
    "config = get_config(config_path)\n",
    "\n",
    "dataset_class = TrajectoryDatasetV2 # if config['study_generalizability'] else TrajectoryDataset\n",
    "data_dir = 'data'\n",
    "\n",
    "# Load datasets\n",
    "# data_dir = \n",
    "train_path = PATH / f'{data_dir}/dataset/train.h5'\n",
    "valid_path = PATH / f'{data_dir}/dataset/valid.h5'\n",
    "trainset = dataset_class(train_path, window=config['window'])\n",
    "validset = dataset_class(valid_path, window=config['window'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcadd96f-8407-4a4c-bea9-dde75f5bb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "runpath = PATH / 'runs_eval' / f\"run_{timestamp}_{'eval_win1'}\"\n",
    "runpath.mkdir(parents=True, exist_ok=True)\n",
    "config['runpath'] = runpath\n",
    "config['num_workers'] = int(subprocess.check_output(['nproc']).strip())\n",
    "config['marginal_prob_std_fn'] = functools.partial(marginal_prob_std, sigma=config['sigma'])\n",
    "\n",
    "flow_prior = get_flow_prior(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f207820c-e544-453c-93d6-29ee4c8262ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_observations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 148\u001b[0m, in \u001b[0;36mcreate_observations\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    146\u001b[0m         x_tensor \u001b[38;5;241m=\u001b[39m _to_torch(arr, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    147\u001b[0m         obs \u001b[38;5;241m=\u001b[39m observation_generator(x_tensor, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma_obs_hi\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 148\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new observations file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservations created successfully.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/h5py/_hl/dataset.py:47\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Convert data to a C-contiguous ndarray\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty):\n\u001b[0;32m---> 47\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43marray_for_new_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecified_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Validate shape\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/h5py/_hl/base.py:118\u001b[0m, in \u001b[0;36marray_for_new_object\u001b[0;34m(data, specified_dtype)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     as_dtype \u001b[38;5;241m=\u001b[39m guess_dtype(data)\n\u001b[0;32m--> 118\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# In most cases, this does nothing. But if data was already an array,\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# and as_dtype is a tagged h5py dtype (e.g. for an object array of strings),\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# asarray() doesn't replace its dtype object. This gives it the tagged dtype:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "source": [
    "create_observations(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50f5c82d-2709-4ac2-977d-2e7f85501331",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = './data/dataset/obs_L15_win1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     gt \u001b[38;5;241m=\u001b[39m gt[:config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;66;03m# shape: (L+1, 3)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Observation\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath_dataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/obs_L\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_win\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwindow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# TODO: deal with window size.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m][n])\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# shape: (L+1, 1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m flow_prior \u001b[38;5;241m=\u001b[39m get_flow_prior(config)\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/groups/astuart/sotakao/miniconda3/envs/flowdas/lib/python3.10/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = './data/dataset/obs_L15_win1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Ground truth\n",
    "n = 0\n",
    "path_dataset = config[f'path_dataset']\n",
    "with h5py.File(f'{path_dataset}/test.h5', mode='r') as f:\n",
    "    arr = f['data'][n][...]            # shape e.g. (L, 3)\n",
    "    gt = _to_torch(arr, dtype=np.float32)\n",
    "    # gt = torch.from_numpy(f['data'][n]).to(config['device']) \n",
    "    gt = gt[:config['LT']+config['window']] # shape: (L+1, 3)\n",
    "\n",
    "# Observation\n",
    "with h5py.File(f'{path_dataset}/obs_L{config[\"LT\"]}_win{config[\"window\"]}.h5', mode='r') as f:\n",
    "    # TODO: deal with window size.\n",
    "    arr = f['obs'][n][...]            # shape e.g. (L, 3)\n",
    "    obs = _to_torch(arr, dtype=np.float32)\n",
    "    # obs = torch.from_numpy(f['obs'][n]).to(config['device']) # shape: (L+1, 1)\n",
    "\n",
    "flow_prior = get_flow_prior(config)\n",
    "\n",
    "gt_win, obs_win = gt, obs\n",
    "initial_cond = gt[:config['window']].reshape(1, -1)\n",
    "\n",
    "# Monte Carlo sampling\n",
    "cond_win = []\n",
    "cond_win.append(initial_cond)\n",
    "est_all_win = [gt[l, :].unsqueeze(0) for l in range(config['window'])]\n",
    "\n",
    "x_t_gen = Euler_Maruyama_sampler(\n",
    "                flow_prior,\n",
    "                num_steps=config['num_steps'],\n",
    "                device=config['device'],\n",
    "                base=est_all_win[i+config['window']-1],\n",
    "                cond=cond_win[i],\n",
    "                measurement=obs_win[i+1, :],\n",
    "                noisy_level=config['sigma_obs_hi'],\n",
    "                MC_times=config['N_MC'], \n",
    "                batch_size=1, # TODO: why batch size is 1?\n",
    "                step_size=config['step_size'] \n",
    "            ) # shape: (B, 3*window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75d9c6f4-0134-4d11-b1bc-4c858e858d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e44b51d-eb4c-452e-9014-2b6a056957fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/dataset'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85c6a6-2b51-4cd3-a44d-3cff8474f180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowdas",
   "language": "python",
   "name": "flowdas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
